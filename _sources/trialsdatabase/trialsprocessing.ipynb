{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mysterious-visibility",
   "metadata": {},
   "source": [
    "# Pisces trial data pre-processing and database upload\n",
    "\n",
    "## This document describes the steps required to pre-process the Pisces trial data for upload\n",
    "\n",
    "The ongoing Pisces trials for both M&S and Tesco projects use the same methodology, so it makes sense to store the data in the same database. Each cruise however has multiple data sources, and these need to be screened, processed, and standardized into a form that enables the data to be uploaded to a database for storage and to enable as painless an import procedure into an analysis program. In an ideal world, we would have machine-collected data that can be simply sucked into a data table as-is. In the real world, Sod's law prevails so we have to do quite a bit of wrangling to get the data into the right format.  \n",
    "\n",
    "\n",
    "There are four different sources of data:  \n",
    "1. **CatchApp**. Each vessel has CatchApp on an iPad, and this is our primary record of which tows were carried out on any given day. The CatchApp data are downloaded from www.succorfish.net. Some of the fields require manual entry of character field values, so unfortunately we have to screen these during pre-processing. But in general CatchApp is a great tool and vast improvement over handwritten sheets. CatchApp is split into two tables:\n",
    " - Vessel, date, Pisces status. This forms the basis of our core identifier table\n",
    " - Catch species and retained/discarded weights. This forms the catch table\n",
    "2. **Followmee**. This is another great tool, and is a GPS tracker on the iPad. It pings approximately every 2-10 minutes, and from Followmee we get the start and end of each haul.\n",
    "  - The track data is uploaded with only a few additions directly to the database. It has a date-time stamp, so is readily queryable.  \n",
    "3. **Effort**. CatchApp provides a field for haul duration, but the estimate is not particularly reliable. Consequently, I record the gear-in/gear-out time and position manually by going through the tracks day by day, and work out from the vessel speed and course whether they are towing or not. Eventually I can train a model to predict this. It wouldn't be hard. When the vessel slows to ~ 3 kts it is towing. This is easier to do for the larger vessels. In the case of *Eilidh Anne* we can't tell whether she is fishing or not. At best we can see the start of the first haul of the day, and the end of the last haul.  \n",
    "4. **Deck photos**. These will hopefully form the basis of a hopper catch calibration. Currently the skipper sends a photo of the catch to the project phone via WhatsApp. Unfortunately this does not contain the EXIF (image information) of when the photo was actually taken. Additionally, the photo filename is not consistent and changes with different downloads due to iOS nonsense. To deal with this, a copy of the photo is renamed to a format that identifies where and when the image was taken. The image is stored in a data directory, and the filename entered into a database table. This enables retrieval of the photo using the sample identification variable. Note: Photos are typically taken of the hopper, but sometimes additional photos of the catch sample are taken. This is taken care of in the file renaming step.  Of all the processes, this is probably the most manually intensive and error-prone. I have automated it as much as possible.  \n",
    "5. **Catch subsample bulk and fish length**. These data are collected only when there is an observer on board. A subsample of the catch (one bucket) is collected, and different elements of the catch weighed and the fishes are measured (total length rounded down to the nearest centimeter, except for skates which are measured tip to tip). Currently these data are entered in a Google Sheet. The format itself is not conducive for direct read-in to the database. However, its current form is tractable for the person on the boat entering it, so it is quicker to leave that system in place and modify it later. The sequence is:\n",
    "  - Restructure a copy of the data into individual columns (one per variable)\n",
    "  - Read the individual sheets corresponding to an individual haul into R, making sure the haul sample identifiers are correctly specified\n",
    "  - Split into fish length and sample weight database tables\n",
    "   \n",
    "```{note}\n",
    "Sometimes we need to modify the database structure to accommodate idiosyncrasies in the data. For example, Eilidh Anne effort measurements are *per day*, with a fudge factor for net retrieval and re-deployment. This requires a separate database table. It is what it is...\n",
    "```  \n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "The hard code samples (well, as much as they can be hard given I have to continually modify them) are written in Jupyter notebooks. As things settle in and I cover the range of idiosyncrasies, I will formalise these a bit more into a closed repository on Github.\n",
    "```\n",
    "````\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "As we add vessels protocols may change.For example, in the first Golden Ray trial a paired design was tested. This requires a change to the trialID table. RDBMS are structured, but we can add a degree of flexibility where required.\n",
    "```\n",
    "````\n",
    "\n",
    "The pre-processing steps take the five data sources and screen, standardize, and restructure to give eight csv files. These files are then read directly into the database using the generic sql statement\n",
    "\n",
    "```\n",
    "LOAD DATA INFILE csv \n",
    "IGNORE INTO tablename\n",
    "```\n",
    "There will be more of that later in the next section.  \n",
    "This gives us the following schema (links are omitted for clarity, and because not all tables have primary keys):\n",
    "\n",
    "```{figure} PiscesTrialDatabaseSchema.png\n",
    "```\n",
    "\n",
    "## Reading and splitting the CatchApp data\n",
    "Each vessel has its own Jupyter Notebook in which I have to accumulate and deal with vesel-specific idiosyncrasies. What follows is a broad-brush treatment of the steps required.  \n",
    "\n",
    "First step is to read in the raw CatchApp file and deal with specific issues like specifying which project the data should be assigned to, make a primary key sampleID, add a variable in (Year, Month, Day) format that MySQL can read as a date variable, tidu up some variable names, and synonymise any typos.  \n",
    "To give you some idea of what's involved, this is the code from Virtuous after the first trip (click to show the code):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-construction",
   "metadata": {},
   "source": [
    "{\n",
    "    \"tags\": [\n",
    "        \"hide_input\",\n",
    "    ]\n",
    "}\n",
    "```\n",
    "infile <- './Data/VIRTUOUS-FR253-.csv'\n",
    "df <- read.csv(infile, stringsAsFactors=F)\n",
    "\n",
    "#Rename variables for consistency\n",
    "\n",
    "df$Project <- 'M&S' \n",
    "df$Bait <- trimws(df$Bait)\n",
    "df$Bait[df$Bait == 'No lights'] <- 'No light'\n",
    "df$Bait[df$Bait == 'White light constant bright'] <- 'White, constant, bright'\n",
    "df$Bait[df$Bait == 'White on bright smp'] <- 'White, constant, bright'\n",
    "\n",
    "df$Position <- 'SMP'\n",
    "df$Activity <- gsub(' ', '', df$Activity)\n",
    "df$Asset[df$Asset == 'VIRTUOUS FR253'] <- 'Virtuous'\n",
    "\n",
    "\n",
    "#Generate sample PK\n",
    "df$SampleID <- paste(df$Asset, df$Start.Date, df$Activity, sep='_')\n",
    "\n",
    "#Don't generate Date values for MYSQL\n",
    "#df$Date_Start <- as.Date(df$Start.Date, format = \"%d/%m/%Y\")\n",
    "#df$Date_End <- as.Date(df$End.Date, format = \"%d/%m/%Y\")\n",
    "\n",
    "#Generate Lat-Longs\n",
    "df <- cbind(df, strcapture(\"(.*),(.*)\", as.character(df$Location), data.frame(Latitude = \"\", Longitude = \"\")))\n",
    "df$Latitude <- as.numeric(df$Latitude)\n",
    "df$Longitude <- as.numeric(df$Longitude)\n",
    "\n",
    "df$Soak.tow.time <- df$Soak...tow.time \n",
    "df$Soak...tow.time <- NULL\n",
    "\n",
    "df$Retained.Weight <- df$Retained.weight \n",
    "df$Retained.weight <- NULL\n",
    "\n",
    "df$Returned.Reason <- df$Returned.reason \n",
    "df$Returned.reason <- NULL\n",
    "\n",
    "df$Retained.Number <- df$Retained.No. \n",
    "df$Retained.No. <- NULL\n",
    "\n",
    "df$Gear.No <- df$Gear.No. \n",
    "df$Gear.No. <- NULL\n",
    "\n",
    "\n",
    "# MySQL reads year-month-day into dates\n",
    "df$year <- substr(df$Start.Date, 7, 11)\n",
    "df$month <- substr(df$Start.Date, 4, 5)\n",
    "df$day <- substr(df$Start.Date, 1, 2)\n",
    "\n",
    "df$YearMonthDay <- paste(df$year, df$month, df$day, sep='-')\n",
    "\n",
    "df$year <- NULL\n",
    "df$month <- NULL\n",
    "df$day <- NULL\n",
    "\n",
    "#Parse into constituent light elements\n",
    "\n",
    "for (i in 1:nrow(df)) {\n",
    "if (df[i,'Bait'] == 'No light'){\n",
    "       df[i,'Light'] <- 'Off'\n",
    "       df[i,'Colour'] <- 'None'\n",
    "       df[i,'Flash'] <- 'None'\n",
    "       df[i,'Intensity'] <- 'None'}\n",
    "\n",
    "else if (df[i,'Bait'] == 'White, constant, bright'){\n",
    "       df[i,'Light'] <- 'On'\n",
    "       df[i,'Colour'] <- 'White'\n",
    "       df[i,'Flash'] <- 'Constant'\n",
    "       df[i,'Intensity'] <- 'Bright'\n",
    "    }\n",
    "    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-overview",
   "metadata": {},
   "source": [
    "Of course as more typos etc. accumulate, we need to constantly update the notebook to accommodate this.  \n",
    "But once this is done, we can easily write two of the important tables: The trialID table, and the catchData table:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-fishing",
   "metadata": {},
   "source": [
    "{\n",
    "    \"tags\": [\n",
    "        \"hide_input\",\n",
    "    ]\n",
    "}\n",
    "```\n",
    "trialID <- unique(df[,c(\"SampleID\", \"Project\", \"Asset\", \"YearMonthDay\", \"Start.Date\", \"End.Date\", \"Location\", \"Activity\", \n",
    "\"Gear\", \"Gear.Type\", \"Gear.No\", \"Bait\", \"Gear.info\",\n",
    "\"Latitude\", \"Longitude\", \"Soak.tow.time\", \"Position\", \"Light\", \"Colour\", \n",
    "\"Flash\", \"Intensity\")])\n",
    "trialID <- trialID[order(trialID$SampleID),]\n",
    "write.csv(trialID, file='./Data/trialIDVirt1.csv', row.names=F)\n",
    "\n",
    "catchData <- df[c(\"SampleID\",\"Species\", \"Returned.Weight\", \"Returned.Number\", \"Retained.Weight\", \n",
    "           \"Retained.Number\")]\n",
    "\n",
    "catchData <- setDF(setDT(catchData)[, lapply(.SD, sum, na.rm=TRUE),\n",
    "                    by=c('SampleID', 'Species'), \n",
    "                    .SDcols=c(\"Returned.Weight\", \"Returned.Number\", \"Retained.Weight\",  \"Retained.Number\")])\n",
    "\n",
    "#I need to make a primary (unique) key to avoid uploading duplicates\n",
    "\n",
    "catchData$CountID <- paste(catchData$SampleID, catchData$Species, sep='_')\n",
    "catchData <- catchData[c(\"SampleID\", \"CountID\", \"Species\", \"Returned.Weight\", \"Returned.Number\", \"Retained.Weight\",  \"Retained.Number\")] \n",
    "write.csv(catchData, file='./Data/catchDataVirt1.csv', row.names=F)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-andrews",
   "metadata": {},
   "source": [
    "## Reading the track data from Followmee\n",
    "\n",
    "This is probably the least painful component. All I need to do is read the saved csv from Followmee, and add some identifiers to the data set. Note in the case of Virtuous, I had to delete the section of Tom's car trip from Hull to Whitby.\n",
    "\n",
    "```\n",
    "#Change directory/filename as required\n",
    "track <- read.csv('./Data/VirtuoustrackLastRecord.csv', stringsAsFactors=F)\n",
    "track$ID <- \"Virtuous\"\n",
    "track$keyID <- paste0(track$ID, track$Date)\n",
    "\n",
    "# Delete the first part\n",
    "track$DTime <- as_datetime(track$Date)\n",
    "track <- subset(track, DTime > as_datetime('2021-04-13 04:00:00 PM'))\n",
    "track$DTime <- NULL\n",
    "write.csv(track, file='./Data/trackDataVirt1.csv',\n",
    "          na = \"\",\n",
    "          row.names=F)\n",
    "```\n",
    "\n",
    "## Reading the manually entered Effort data\n",
    "At present I can't reliably identify trawl start and stop from Followmee. So, I manually examine the track and cop-paste the position and time from what I can see is the beginning end of the trawl. This is entered in a CSV file, and read in a written to another database table. Note I measure the trawl duration in both hours and minutes, although we will use hours for the CPUE correction:  \n",
    "\n",
    "```\n",
    "df <- read.csv('./Data/VirtuousEffort1.csv', stringsAsFactors=F)\n",
    "df$SampleID <- paste(df$Asset, as.character(format(dmy(df$StartDate), \"%d/%m/%Y\")), df$Activity, sep='_')\n",
    "df$TowTimeMinutes <- difftime(ymd_hms(df$TrawlEndTime), ymd_hms(df$TrawlStartTime), units=\"mins\")\n",
    "df$TowTimeHours <- difftime(ymd_hms(df$TrawlEndTime), ymd_hms(df$TrawlStartTime), units=\"hours\")\n",
    "\n",
    "df$tempdat <- as.character(format(dmy(df$StartDate), \"%d/%m/%Y\"))\n",
    "df$tempdat\n",
    "\n",
    "# MySQL reads year-month-day into dates\n",
    "df$year <- substr(df$tempdat, 7, 10)\n",
    "df$month <- substr(df$tempdat, 4, 5)\n",
    "df$day <- substr(df$tempdat, 1, 2)\n",
    "\n",
    "df$YearMonthDay <- paste(df$year, df$month, df$day, sep='-')\n",
    "\n",
    "df$year <- NULL\n",
    "df$month <- NULL\n",
    "df$day <- NULL\n",
    "df$tempdat <- NULL\n",
    "```\n",
    "\n",
    "For Virtuous and Golden Ray this value directly corresponds with the trialID key variable. Ideally we would need only the one effort table. However... we can't do this for Eilidh Anne because we only have the beginning and start of the day - not the haul. So for Eilidh Anne we have a separate table.  \n",
    "\n",
    "## Renaming and loading the image filenames\n",
    "This step is definitely one that you need to keep on top of. Typically there will be four hauls per day, so the number of photos quickly stacks up. Manually renaming the images is not a viable option. There is too much potential for silly typo errors.  \n",
    "The approach I use is to read the unique SampleID variable from the trialID table, and directly parse the identifier into the filename. To do this, we need the images to be renamed to be in a separate directory. I get a listing of the filenames, then go through the WhatsApp messages to make sure the images are correctly lined with the SampleID variables.  \n",
    "For the most part these are photos of the hopper. In some cases they are elsewhere (eg. the sample), so I need to add an identifier into the filename.  \n",
    "\n",
    "```{note}\n",
    "It is possible to store the image itself i a MySQL database as a BLOB (Binary Large Object) entry. However nobody in database land recommends it, so I simply save the filename in the database table to allow retrieval of the correct photo.\n",
    "```\n",
    "\n",
    "The simplified coding looks a bit like this...\n",
    "\n",
    "```\n",
    "frompath <- './2021_04_VirtuousPhotos/'\n",
    "topath <- './2021_04_VirtuousPhotosRenamed/'\n",
    "list.files('frompath')\n",
    "trialID <- trialID$SampleID\n",
    "\n",
    "inittable <- data.frame(SampleID=character(0), SampleImageFile=character(0))\n",
    "inittable$SampleID <- as.character(inittable$SampleID)\n",
    "inittable$SampleImageFile <- as.character(inittable$SampleImageFile)\n",
    "\n",
    "rename <- function(FROM, OF, IDNUM){\n",
    "    file1 <- paste0(frompath, FROM)\n",
    "    id1 <- trialID[IDNUM, 'SampleID']\n",
    "    file2 <- paste(id1, OF, sep='_')\n",
    "    file2 <- paste(file2, 'JPG', sep='.')\n",
    "    file2 <- gsub(\"[[:space:]]\", \"_\", file2)\n",
    "    file2 <- gsub(\"/\", \"_\", file2)\n",
    "    file2 <- paste0(topath, file2)\n",
    "    !file.copy(from=file1, to=file2)\n",
    "}\n",
    "\n",
    "rename('IMG_0009.JPG', 'Hopper', 2)\n",
    "# Do this for all the image files in the list\n",
    "\n",
    "# Now write the new image names to a csv to be imported to the database\n",
    "filelist <- data.frame(list.files('./2021_04_VirtuousPhotosRenamed/'))\n",
    "names(filelist) <- 'file'\n",
    "filelist$SampleID <- substr(filelist$file, 1, 25)\n",
    "filelist$SampleID <- gsub(\"_\", \"/\", filelist$SampleID)\n",
    "filelist$SampleID <- gsub(\"s/\", \"s_\", filelist$SampleID)\n",
    "filelist$SampleID <- gsub(\"/H\", \"_H\", filelist$SampleID)\n",
    "\n",
    "filelist$PhotoLocation <- substr(filelist$file, 27, 32)\n",
    "\n",
    "filelist <- filelist[c('SampleID', 'file', 'PhotoLocation')]\n",
    "filelist\n",
    "write.csv(filelist, file='./Data/deckPhotoFileVirt1.csv',\n",
    "         row.names=F)\n",
    "```\n",
    "\n",
    "Pretty ugly, huh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-crawford",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
